"""
ðŸ¤– ADVANCED AI ORCHESTRATION ENGINE
O5 Elite Level LLM Coordination Platform

This module provides sophisticated AI orchestration capabilities that exceed
human-level software development coordination. It implements advanced model routing,
intelligent load balancing, cost optimization, and multi-modal AI coordination.

Features:
- Multi-provider LLM orchestration (OpenAI, Anthropic, Google, Azure)
- Intelligent model selection and routing
- Advanced prompt engineering and optimization
- Real-time model performance monitoring
- Cost optimization algorithms
- Failure recovery and fallback mechanisms
- Context management and memory systems
- Agent swarm coordination
- Advanced reasoning capabilities
"""

import asyncio
import json
import time
from typing import Dict, List, Any, Optional, Union, AsyncGenerator
from dataclasses import dataclass, field
from enum import Enum
import logging
from datetime import datetime, timedelta
import uuid
import numpy as np
from collections import defaultdict, deque
import structlog

# AI Provider Imports
import openai
import anthropic
import google.generativeai as genai
from azure.ai.ml import MLClient

# Core System Imports
from pydantic import BaseModel, Field, validator
from core.security import SecurityManager
from core.cache import DistributedCacheManager
from core.data_manager import EnterpriseDataManager
from config.enterprise_config import EnterpriseConfig

logger = structlog.get_logger(__name__)

class ModelProvider(Enum):
    """Available AI model providers"""
    OPENAI = "openai"
    ANTHROPIC = "anthropic"
    GOOGLE = "google"
    AZURE = "azure"
    COHERE = "cohere"
    HUGGINGFACE = "huggingface"

class ModelCapability(Enum):
    """AI model capabilities"""
    TEXT_GENERATION = "text_generation"
    CODE_GENERATION = "code_generation"
    ANALYSIS = "analysis"
    REASONING = "reasoning"
    VISION = "vision"
    FUNCTION_CALLING = "function_calling"
    EMBEDDINGS = "embeddings"
    MULTIMODAL = "multimodal"

class OrchestrationStrategy(Enum):
    """AI orchestration strategies"""
    SINGLE_MODEL = "single_model"
    MULTI_MODEL_PARALLEL = "multi_model_parallel"
    MULTI_MODEL_SEQUENTIAL = "multi_model_sequential"
    AGENT_SWARM = "agent_swarm"
    HIERARCHICAL = "hierarchical"
    CONSENSUS = "consensus"

@dataclass
class ModelMetrics:
    """Performance metrics for AI models"""
    response_time: float = 0.0
    success_rate: float = 1.0
    cost_per_token: float = 0.0
    quality_score: float = 1.0
    reliability_score: float = 1.0
    last_updated: datetime = field(default_factory=datetime.now)
    request_count: int = 0
    error_count: int = 0

@dataclass
class ModelConfig:
    """Configuration for an AI model"""
    provider: ModelProvider
    model_name: str
    capabilities: List[ModelCapability]
    max_tokens: int
    temperature: float = 0.7
    top_p: float = 1.0
    frequency_penalty: float = 0.0
    presence_penalty: float = 0.0
    cost_per_input_token: float = 0.0
    cost_per_output_token: float = 0.0
    rate_limit_rpm: int = 1000
    rate_limit_tpm: int = 100000
    context_window: int = 4096
    supports_streaming: bool = True
    supports_functions: bool = False
    priority: int = 1  # Higher numbers = higher priority

class OrchestrationRequest(BaseModel):
    """Request for AI orchestration"""
    
    # Core Request Data
    message: str = Field(..., description="User message/prompt")
    context: Optional[Dict[str, Any]] = Field(default=None, description="Additional context")
    user_id: str = Field(..., description="User identifier")
    session_id: Optional[str] = Field(default=None, description="Session identifier")
    
    # Orchestration Configuration
    strategy: OrchestrationStrategy = Field(default=OrchestrationStrategy.SINGLE_MODEL, description="Orchestration strategy")
    preferred_models: Optional[List[str]] = Field(default=None, description="Preferred model names")
    required_capabilities: List[ModelCapability] = Field(default=[], description="Required capabilities")
    
    # Performance Requirements
    max_response_time: Optional[float] = Field(default=30.0, description="Maximum response time in seconds")
    max_cost: Optional[float] = Field(default=None, description="Maximum cost in USD")
    quality_threshold: float = Field(default=0.8, description="Minimum quality threshold")
    
    # Advanced Features
    enable_streaming: bool = Field(default=False, description="Enable streaming response")
    enable_function_calling: bool = Field(default=False, description="Enable function calling")
    enable_vision: bool = Field(default=False, description="Enable vision capabilities")
    
    # Collaboration Features
    collaboration_enabled: bool = Field(default=False, description="Enable collaboration features")
    share_with_team: bool = Field(default=False, description="Share with team members")
    
    # Metadata
    request_id: str = Field(default_factory=lambda: str(uuid.uuid4()))
    timestamp: datetime = Field(default_factory=datetime.now)
    
    @validator('strategy')
    def validate_strategy(cls, v):
        if not isinstance(v, OrchestrationStrategy):
            try:
                return OrchestrationStrategy(v)
            except ValueError:
                raise ValueError(f"Invalid orchestration strategy: {v}")
        return v

class OrchestrationResponse(BaseModel):
    """Response from AI orchestration"""
    
    # Core Response Data
    content: str = Field(..., description="Generated content")
    metadata: Dict[str, Any] = Field(default_factory=dict, description="Response metadata")
    
    # Performance Metrics
    response_time: float = Field(..., description="Response time in seconds")
    total_cost: float = Field(default=0.0, description="Total cost in USD")
    quality_score: float = Field(default=1.0, description="Quality score")
    
    # Model Information
    models_used: List[str] = Field(default=[], description="Models used in orchestration")
    primary_model: str = Field(..., description="Primary model used")
    strategy_used: OrchestrationStrategy = Field(..., description="Orchestration strategy used")
    
    # Token Usage
    input_tokens: int = Field(default=0, description="Input tokens used")
    output_tokens: int = Field(default=0, description="Output tokens generated")
    
    # Status Information
    success: bool = Field(default=True, description="Whether request was successful")
    error_message: Optional[str] = Field(default=None, description="Error message if failed")
    
    # Request Tracking
    request_id: str = Field(..., description="Original request ID")
    timestamp: datetime = Field(default_factory=datetime.now)

class LLMProvider:
    """Base class for LLM providers"""
    
    def __init__(self, config: ModelConfig, api_key: str):
        self.config = config
        self.api_key = api_key
        self.metrics = ModelMetrics()
        self._rate_limiter = deque()
        
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response from the model"""
        raise NotImplementedError
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Generate streaming response from the model"""
        raise NotImplementedError
    
    async def get_embeddings(self, text: str) -> List[float]:
        """Get embeddings for text"""
        raise NotImplementedError
    
    def update_metrics(self, response_time: float, success: bool, cost: float = 0.0):
        """Update model performance metrics"""
        self.metrics.request_count += 1
        if not success:
            self.metrics.error_count += 1
        
        # Update moving averages
        alpha = 0.1  # Smoothing factor
        self.metrics.response_time = alpha * response_time + (1 - alpha) * self.metrics.response_time
        self.metrics.success_rate = alpha * (1.0 if success else 0.0) + (1 - alpha) * self.metrics.success_rate
        
        if cost > 0:
            self.metrics.cost_per_token = cost / max(1, self.metrics.request_count)
        
        self.metrics.last_updated = datetime.now()
    
    async def check_rate_limit(self) -> bool:
        """Check if request is within rate limits"""
        now = time.time()
        
        # Remove old requests outside the window
        while self._rate_limiter and now - self._rate_limiter[0] > 60:
            self._rate_limiter.popleft()
        
        # Check if we're under the rate limit
        return len(self._rate_limiter) < self.config.rate_limit_rpm

class OpenAIProvider(LLMProvider):
    """OpenAI provider implementation"""
    
    def __init__(self, config: ModelConfig, api_key: str):
        super().__init__(config, api_key)
        self.client = openai.AsyncOpenAI(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response using OpenAI"""
        
        if not await self.check_rate_limit():
            raise Exception("Rate limit exceeded")
        
        start_time = time.time()
        
        try:
            response = await self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
                temperature=kwargs.get("temperature", self.config.temperature),
                top_p=kwargs.get("top_p", self.config.top_p),
                frequency_penalty=kwargs.get("frequency_penalty", self.config.frequency_penalty),
                presence_penalty=kwargs.get("presence_penalty", self.config.presence_penalty)
            )
            
            content = response.choices[0].message.content
            response_time = time.time() - start_time
            
            # Calculate cost
            input_cost = response.usage.prompt_tokens * self.config.cost_per_input_token
            output_cost = response.usage.completion_tokens * self.config.cost_per_output_token
            total_cost = input_cost + output_cost
            
            self.update_metrics(response_time, True, total_cost)
            self._rate_limiter.append(time.time())
            
            return content
            
        except Exception as e:
            response_time = time.time() - start_time
            self.update_metrics(response_time, False)
            raise e
    
    async def generate_stream(self, prompt: str, **kwargs) -> AsyncGenerator[str, None]:
        """Generate streaming response using OpenAI"""
        
        if not await self.check_rate_limit():
            raise Exception("Rate limit exceeded")
        
        try:
            response = await self.client.chat.completions.create(
                model=self.config.model_name,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
                temperature=kwargs.get("temperature", self.config.temperature),
                stream=True
            )
            
            async for chunk in response:
                if chunk.choices[0].delta.content:
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            logger.error("OpenAI streaming failed", error=str(e))
            raise e

class AnthropicProvider(LLMProvider):
    """Anthropic Claude provider implementation"""
    
    def __init__(self, config: ModelConfig, api_key: str):
        super().__init__(config, api_key)
        self.client = anthropic.AsyncAnthropic(api_key=api_key)
    
    async def generate(self, prompt: str, **kwargs) -> str:
        """Generate response using Anthropic Claude"""
        
        if not await self.check_rate_limit():
            raise Exception("Rate limit exceeded")
        
        start_time = time.time()
        
        try:
            response = await self.client.messages.create(
                model=self.config.model_name,
                max_tokens=kwargs.get("max_tokens", self.config.max_tokens),
                temperature=kwargs.get("temperature", self.config.temperature),
                messages=[{"role": "user", "content": prompt}]
            )
            
            content = response.content[0].text
            response_time = time.time() - start_time
            
            # Calculate cost
            input_cost = response.usage.input_tokens * self.config.cost_per_input_token
            output_cost = response.usage.output_tokens * self.config.cost_per_output_token
            total_cost = input_cost + output_cost
            
            self.update_metrics(response_time, True, total_cost)
            self._rate_limiter.append(time.time())
            
            return content
            
        except Exception as e:
            response_time = time.time() - start_time
            self.update_metrics(response_time, False)
            raise e

class LLMOrchestrator:
    """
    ðŸ¤– ADVANCED AI ORCHESTRATION ENGINE
    
    This is the core AI orchestration system that coordinates multiple LLM providers
    to deliver enterprise-grade AI capabilities. It implements sophisticated routing,
    optimization, and coordination strategies.
    """
    
    def __init__(
        self,
        config: EnterpriseConfig,
        security_manager: SecurityManager,
        cache_manager: DistributedCacheManager,
        data_manager: EnterpriseDataManager
    ):
        self.config = config
        self.security_manager = security_manager
        self.cache_manager = cache_manager
        self.data_manager = data_manager
        
        # Provider registry
        self.providers: Dict[str, LLMProvider] = {}
        self.model_configs: Dict[str, ModelConfig] = {}
        
        # Performance tracking
        self.request_history: deque = deque(maxlen=10000)
        self.model_performance: Dict[str, ModelMetrics] = {}
        
        # Advanced features
        self.context_memory: Dict[str, Any] = {}
        self.agent_swarms: Dict[str, List[str]] = {}
        
        # Cost optimization
        self.daily_costs: Dict[str, float] = defaultdict(float)
        self.cost_budgets: Dict[str, float] = {}
        
        self.initialized = False
    
    async def initialize(self):
        """Initialize the AI orchestrator"""
        
        logger.info("ðŸ¤– Initializing AI Orchestration Engine...")
        
        try:
            # Initialize model configurations
            await self._initialize_model_configs()
            
            # Initialize providers
            await self._initialize_providers()
            
            # Load performance history
            await self._load_performance_history()
            
            # Initialize advanced features
            await self._initialize_advanced_features()
            
            self.initialized = True
            logger.info("âœ… AI Orchestration Engine initialized successfully")
            
        except Exception as e:
            logger.error("âŒ Failed to initialize AI Orchestration Engine", error=str(e))
            raise
    
    async def _initialize_model_configs(self):
        """Initialize model configurations"""
        
        # OpenAI Models
        self.model_configs.update({
            "gpt-4-turbo": ModelConfig(
                provider=ModelProvider.OPENAI,
                model_name="gpt-4-turbo-preview",
                capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING, ModelCapability.CODE_GENERATION],
                max_tokens=4096,
                cost_per_input_token=0.00003,
                cost_per_output_token=0.00006,
                context_window=128000,
                supports_functions=True,
                priority=10
            ),
            "gpt-3.5-turbo": ModelConfig(
                provider=ModelProvider.OPENAI,
                model_name="gpt-3.5-turbo",
                capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.CODE_GENERATION],
                max_tokens=4096,
                cost_per_input_token=0.000001,
                cost_per_output_token=0.000002,
                context_window=16385,
                supports_functions=True,
                priority=5
            )
        })
        
        # Anthropic Models
        self.model_configs.update({
            "claude-3-opus": ModelConfig(
                provider=ModelProvider.ANTHROPIC,
                model_name="claude-3-opus-20240229",
                capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING, ModelCapability.ANALYSIS],
                max_tokens=4096,
                cost_per_input_token=0.000015,
                cost_per_output_token=0.000075,
                context_window=200000,
                priority=9
            ),
            "claude-3-sonnet": ModelConfig(
                provider=ModelProvider.ANTHROPIC,
                model_name="claude-3-sonnet-20240229",
                capabilities=[ModelCapability.TEXT_GENERATION, ModelCapability.REASONING],
                max_tokens=4096,
                cost_per_input_token=0.000003,
                cost_per_output_token=0.000015,
                context_window=200000,
                priority=7
            )
        })
    
    async def _initialize_providers(self):
        """Initialize AI providers"""
        
        # Initialize OpenAI providers
        if self.config.AI.OPENAI_API_KEY:
            for model_name, model_config in self.model_configs.items():
                if model_config.provider == ModelProvider.OPENAI:
                    self.providers[model_name] = OpenAIProvider(model_config, self.config.AI.OPENAI_API_KEY)
        
        # Initialize Anthropic providers
        if self.config.AI.ANTHROPIC_API_KEY:
            for model_name, model_config in self.model_configs.items():
                if model_config.provider == ModelProvider.ANTHROPIC:
                    self.providers[model_name] = AnthropicProvider(model_config, self.config.AI.ANTHROPIC_API_KEY)
        
        logger.info(f"âœ… Initialized {len(self.providers)} AI providers")
    
    async def _load_performance_history(self):
        """Load historical performance data"""
        try:
            performance_data = await self.data_manager.get_model_performance_history()
            for model_name, metrics_data in performance_data.items():
                if model_name in self.providers:
                    self.model_performance[model_name] = ModelMetrics(**metrics_data)
        except Exception as e:
            logger.warning("Could not load performance history", error=str(e))
    
    async def _initialize_advanced_features(self):
        """Initialize advanced AI features"""
        
        # Initialize agent swarms
        self.agent_swarms = {
            "code_generation": ["gpt-4-turbo", "claude-3-opus"],
            "analysis": ["claude-3-opus", "gpt-4-turbo"],
            "reasoning": ["claude-3-opus", "gpt-4-turbo"],
            "general": ["gpt-3.5-turbo", "claude-3-sonnet"]
        }
        
        # Initialize context memory
        self.context_memory = {}
        
        logger.info("âœ… Advanced AI features initialized")
    
    async def process_request(self, request: OrchestrationRequest, user) -> OrchestrationResponse:
        """
        ðŸŽ¯ MAIN ORCHESTRATION ENDPOINT
        
        This is the core method that processes AI requests using advanced
        orchestration strategies to deliver optimal responses.
        """
        
        start_time = time.time()
        
        try:
            # Security validation
            await self.security_manager.validate_ai_request(request, user)
            
            # Check cache first
            cache_key = self._generate_cache_key(request)
            cached_response = await self.cache_manager.get(cache_key)
            
            if cached_response and self.config.AI.ENABLE_RESPONSE_CACHING:
                logger.info("âœ… Returning cached response", request_id=request.request_id)
                return OrchestrationResponse.parse_obj(cached_response)
            
            # Select orchestration strategy
            optimal_strategy = await self._select_orchestration_strategy(request)
            
            # Execute orchestration
            response = await self._execute_orchestration(request, optimal_strategy)
            
            # Cache response
            if self.config.AI.ENABLE_RESPONSE_CACHING:
                await self.cache_manager.set(
                    cache_key, 
                    response.dict(), 
                    expire=3600  # 1 hour
                )
            
            # Update performance metrics
            response_time = time.time() - start_time
            await self._update_performance_metrics(request, response, response_time)
            
            # Track request
            self.request_history.append({
                "request_id": request.request_id,
                "user_id": request.user_id,
                "strategy": optimal_strategy,
                "models_used": response.models_used,
                "response_time": response_time,
                "cost": response.total_cost,
                "timestamp": datetime.now()
            })
            
            logger.info(
                "âœ… AI orchestration completed successfully",
                request_id=request.request_id,
                strategy=optimal_strategy,
                models=response.models_used,
                response_time=response_time,
                cost=response.total_cost
            )
            
            return response
            
        except Exception as e:
            logger.error(
                "âŒ AI orchestration failed",
                request_id=request.request_id,
                error=str(e)
            )
            
            # Return error response
            return OrchestrationResponse(
                content=f"I apologize, but I encountered an error processing your request: {str(e)}",
                response_time=time.time() - start_time,
                primary_model="error",
                strategy_used=OrchestrationStrategy.SINGLE_MODEL,
                success=False,
                error_message=str(e),
                request_id=request.request_id
            )
    
    async def _select_orchestration_strategy(self, request: OrchestrationRequest) -> OrchestrationStrategy:
        """Select optimal orchestration strategy based on request characteristics"""
        
        # Use requested strategy if specified
        if request.strategy != OrchestrationStrategy.SINGLE_MODEL:
            return request.strategy
        
        # Analyze request to determine optimal strategy
        message_length = len(request.message)
        complexity_indicators = [
            "analyze", "compare", "explain", "reason", "complex",
            "multiple", "various", "different", "pros and cons"
        ]
        
        is_complex = any(indicator in request.message.lower() for indicator in complexity_indicators)
        
        # Select strategy based on analysis
        if is_complex and message_length > 500:
            return OrchestrationStrategy.MULTI_MODEL_PARALLEL
        elif request.enable_function_calling:
            return OrchestrationStrategy.AGENT_SWARM
        elif len(request.required_capabilities) > 1:
            return OrchestrationStrategy.MULTI_MODEL_SEQUENTIAL
        else:
            return OrchestrationStrategy.SINGLE_MODEL
    
    async def _execute_orchestration(
        self, 
        request: OrchestrationRequest, 
        strategy: OrchestrationStrategy
    ) -> OrchestrationResponse:
        """Execute orchestration strategy"""
        
        if strategy == OrchestrationStrategy.SINGLE_MODEL:
            return await self._execute_single_model(request)
        elif strategy == OrchestrationStrategy.MULTI_MODEL_PARALLEL:
            return await self._execute_multi_model_parallel(request)
        elif strategy == OrchestrationStrategy.MULTI_MODEL_SEQUENTIAL:
            return await self._execute_multi_model_sequential(request)
        elif strategy == OrchestrationStrategy.AGENT_SWARM:
            return await self._execute_agent_swarm(request)
        elif strategy == OrchestrationStrategy.CONSENSUS:
            return await self._execute_consensus(request)
        else:
            return await self._execute_single_model(request)
    
    async def _execute_single_model(self, request: OrchestrationRequest) -> OrchestrationResponse:
        """Execute single model strategy"""
        
        # Select best model
        model_name = await self._select_optimal_model(request)
        provider = self.providers[model_name]
        
        start_time = time.time()
        
        # Generate response
        content = await provider.generate(request.message)
        
        response_time = time.time() - start_time
        
        return OrchestrationResponse(
            content=content,
            response_time=response_time,
            models_used=[model_name],
            primary_model=model_name,
            strategy_used=OrchestrationStrategy.SINGLE_MODEL,
            total_cost=provider.metrics.cost_per_token * len(content.split()),
            request_id=request.request_id
        )
    
    async def _execute_multi_model_parallel(self, request: OrchestrationRequest) -> OrchestrationResponse:
        """Execute multiple models in parallel and combine results"""
        
        # Select multiple models
        models = await self._select_multiple_models(request, count=3)
        
        # Generate responses in parallel
        tasks = []
        for model_name in models:
            provider = self.providers[model_name]
            tasks.append(self._generate_with_model(provider, request.message))
        
        start_time = time.time()
        responses = await asyncio.gather(*tasks, return_exceptions=True)
        response_time = time.time() - start_time
        
        # Filter successful responses
        successful_responses = [r for r in responses if isinstance(r, str)]
        
        if not successful_responses:
            raise Exception("All models failed to generate responses")
        
        # Combine responses intelligently
        combined_content = await self._combine_responses(successful_responses, request)
        
        total_cost = sum(
            self.providers[model].metrics.cost_per_token * len(content.split())
            for model, content in zip(models, successful_responses)
        )
        
        return OrchestrationResponse(
            content=combined_content,
            response_time=response_time,
            models_used=models,
            primary_model=models[0],
            strategy_used=OrchestrationStrategy.MULTI_MODEL_PARALLEL,
            total_cost=total_cost,
            request_id=request.request_id
        )
    
    async def _generate_with_model(self, provider: LLMProvider, prompt: str) -> str:
        """Generate response with a specific model provider"""
        try:
            return await provider.generate(prompt)
        except Exception as e:
            logger.warning(f"Model {provider.config.model_name} failed", error=str(e))
            raise e
    
    async def _select_optimal_model(self, request: OrchestrationRequest) -> str:
        """Select the optimal model for a request"""
        
        # Filter models by capabilities
        compatible_models = []
        for model_name, model_config in self.model_configs.items():
            if model_name not in self.providers:
                continue
                
            # Check capability requirements
            if request.required_capabilities:
                if not all(cap in model_config.capabilities for cap in request.required_capabilities):
                    continue
            
            # Check cost constraints
            if request.max_cost:
                estimated_tokens = len(request.message.split()) * 2  # Rough estimate
                estimated_cost = estimated_tokens * (model_config.cost_per_input_token + model_config.cost_per_output_token)
                if estimated_cost > request.max_cost:
                    continue
            
            compatible_models.append(model_name)
        
        if not compatible_models:
            # Fallback to default model
            return self.config.AI.DEFAULT_MODEL
        
        # Select based on performance and cost optimization
        best_model = max(
            compatible_models,
            key=lambda m: self._calculate_model_score(m, request)
        )
        
        return best_model
    
    def _calculate_model_score(self, model_name: str, request: OrchestrationRequest) -> float:
        """Calculate model score for selection"""
        
        model_config = self.model_configs[model_name]
        provider = self.providers[model_name]
        
        # Base score from model priority
        score = model_config.priority
        
        # Adjust for performance metrics
        if model_name in self.model_performance:
            metrics = self.model_performance[model_name]
            score *= metrics.success_rate
            score *= metrics.quality_score
            
            # Penalize slow models if time constraint exists
            if request.max_response_time and metrics.response_time > request.max_response_time:
                score *= 0.5
        
        # Adjust for cost if budget constraint exists
        if request.max_cost:
            estimated_tokens = len(request.message.split()) * 2
            estimated_cost = estimated_tokens * (model_config.cost_per_input_token + model_config.cost_per_output_token)
            cost_ratio = estimated_cost / request.max_cost
            score *= (2.0 - cost_ratio)  # Prefer cheaper models
        
        return score
    
    async def _select_multiple_models(self, request: OrchestrationRequest, count: int = 3) -> List[str]:
        """Select multiple models for parallel processing"""
        
        compatible_models = []
        for model_name, model_config in self.model_configs.items():
            if model_name not in self.providers:
                continue
                
            if request.required_capabilities:
                if not all(cap in model_config.capabilities for cap in request.required_capabilities):
                    continue
            
            compatible_models.append(model_name)
        
        # Sort by score and take top N
        scored_models = sorted(
            compatible_models,
            key=lambda m: self._calculate_model_score(m, request),
            reverse=True
        )
        
        return scored_models[:count]
    
    async def _combine_responses(self, responses: List[str], request: OrchestrationRequest) -> str:
        """Intelligently combine multiple responses"""
        
        if len(responses) == 1:
            return responses[0]
        
        # For now, use a simple combination strategy
        # In production, this would use more sophisticated methods
        
        combined = f"Based on multiple AI analysis:\n\n"
        
        for i, response in enumerate(responses, 1):
            combined += f"**Perspective {i}:**\n{response}\n\n"
        
        # Add a synthesis if we have a high-quality model available
        synthesis_model = await self._select_optimal_model(request)
        synthesis_provider = self.providers[synthesis_model]
        
        synthesis_prompt = f"""
        Please synthesize the following AI responses into a single, coherent answer to the user's question: "{request.message}"

        Responses to synthesize:
        {chr(10).join(f"Response {i+1}: {resp}" for i, resp in enumerate(responses))}

        Provide a clear, concise synthesis that captures the best insights from all responses.
        """
        
        try:
            synthesis = await synthesis_provider.generate(synthesis_prompt)
            combined += f"**Synthesis:**\n{synthesis}"
        except Exception as e:
            logger.warning("Failed to generate synthesis", error=str(e))
        
        return combined
    
    def _generate_cache_key(self, request: OrchestrationRequest) -> str:
        """Generate cache key for request"""
        # Create a hash of the request content
        import hashlib
        content = f"{request.message}_{request.strategy}_{request.required_capabilities}"
        return f"ai_response:{hashlib.md5(content.encode()).hexdigest()}"
    
    async def _update_performance_metrics(
        self, 
        request: OrchestrationRequest, 
        response: OrchestrationResponse,
        response_time: float
    ):
        """Update performance metrics"""
        
        # Update cost tracking
        today = datetime.now().date().isoformat()
        self.daily_costs[today] += response.total_cost
        
        # Save performance data
        try:
            await self.data_manager.save_model_performance(
                request.request_id,
                {
                    "models_used": response.models_used,
                    "strategy": response.strategy_used.value,
                    "response_time": response_time,
                    "cost": response.total_cost,
                    "success": response.success,
                    "timestamp": datetime.now()
                }
            )
        except Exception as e:
            logger.warning("Failed to save performance metrics", error=str(e))
    
    async def get_status(self) -> Dict[str, Any]:
        """Get orchestrator status"""
        return {
            "initialized": self.initialized,
            "providers_count": len(self.providers),
            "models_available": list(self.providers.keys()),
            "total_requests": len(self.request_history),
            "daily_cost": self.daily_costs.get(datetime.now().date().isoformat(), 0.0)
        }
    
    async def get_available_models(self) -> List[Dict[str, Any]]:
        """Get available models and their capabilities"""
        models = []
        for model_name, config in self.model_configs.items():
            if model_name in self.providers:
                provider = self.providers[model_name]
                models.append({
                    "name": model_name,
                    "provider": config.provider.value,
                    "capabilities": [cap.value for cap in config.capabilities],
                    "context_window": config.context_window,
                    "cost_per_input_token": config.cost_per_input_token,
                    "cost_per_output_token": config.cost_per_output_token,
                    "success_rate": provider.metrics.success_rate,
                    "average_response_time": provider.metrics.response_time
                })
        
        return models
    
    async def enhance_note_content(self, note_data: Dict[str, Any], user) -> Dict[str, Any]:
        """Enhance note content with AI assistance"""
        
        enhancement_request = OrchestrationRequest(
            message=f"Enhance this note content for better clarity and organization:\n\n{note_data.get('body', '')}",
            user_id=user.id,
            required_capabilities=[ModelCapability.TEXT_GENERATION],
            max_response_time=15.0
        )
        
        response = await self.process_request(enhancement_request, user)
        
        return {
            "enhanced_body": response.content,
            "ai_enhanced": True,
            "enhancement_model": response.primary_model
        }
    
    async def enhance_note_update(self, note_data: Dict[str, Any], user) -> Dict[str, Any]:
        """Enhance note updates with AI suggestions"""
        
        enhancement_request = OrchestrationRequest(
            message=f"Suggest improvements for this note update:\n\n{note_data.get('body', '')}",
            user_id=user.id,
            required_capabilities=[ModelCapability.ANALYSIS],
            max_response_time=10.0
        )
        
        response = await self.process_request(enhancement_request, user)
        
        return {
            "suggestions": response.content,
            "ai_analysis": True
        }
    
    async def advanced_orchestration(self, config: Dict[str, Any], user) -> Dict[str, Any]:
        """Advanced orchestration with custom configurations"""
        
        # This would implement more sophisticated orchestration patterns
        # For now, return a placeholder response
        
        return {
            "message": "Advanced orchestration capabilities are being processed",
            "status": "processing",
            "estimated_completion": "2-3 minutes"
        }
    
    async def health_check(self) -> Dict[str, Any]:
        """Health check for the orchestrator"""
        
        healthy_providers = 0
        total_providers = len(self.providers)
        
        for provider in self.providers.values():
            if provider.metrics.success_rate > 0.8:
                healthy_providers += 1
        
        health_ratio = healthy_providers / max(1, total_providers)
        
        return {
            "healthy": health_ratio >= 0.7,
            "providers_healthy": healthy_providers,
            "providers_total": total_providers,
            "health_ratio": health_ratio,
            "last_check": datetime.now()
        }
    
    async def shutdown(self):
        """Gracefully shutdown the orchestrator"""
        logger.info("ðŸ”„ Shutting down AI Orchestration Engine...")
        
        # Save performance metrics
        try:
            performance_data = {
                model_name: provider.metrics.__dict__
                for model_name, provider in self.providers.items()
            }
            await self.data_manager.save_performance_snapshot(performance_data)
        except Exception as e:
            logger.warning("Failed to save performance snapshot", error=str(e))
        
        # Clear memory
        self.providers.clear()
        self.request_history.clear()
        self.context_memory.clear()
        
        logger.info("âœ… AI Orchestration Engine shutdown complete")